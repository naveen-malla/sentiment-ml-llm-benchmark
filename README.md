# Sentiment ML vs LLM Benchmark

## 1. Project Overview
- Compare traditional ML (TF–IDF + linear classifiers) against a lightweight transformer (DistilBERT) on a cleaned Twitter-brand sentiment dataset.
- Full codebase lives in `main.py`, with classical pipelines in `traditional_models.py:21` and transformer training in `transformer_models.py:37`.
- Reproducible data preparation and reporting scripts ensure every experiment uses leakage-free splits and can regenerate metrics/figures.

## 2. Dataset Preparation
- Raw Kaggle splits (`data/twitter_training.csv`, `data/twitter_validation.csv`) contained duplicated tweet IDs, blank texts, and shared items between train/val.
- `prepare_data.py:11` merges both files, filters valid labels, drops blanks, deduplicates (`basic_clean`), and splits by unique `tweet_id` into 80/10/10 train/val/test.
- Cleaned outputs stored in `data/processed/` and auto-generated by `preprocessing.py:57` when missing.
- Split sizes and sentiment distribution (from `reports/dataset_stats.json`):
  - Train: 46 037 tweets (Negative 17 091, Positive 15 250, Neutral 13 696)
  - Validation: 5 783 tweets (Negative 2 194, Positive 1 953, Neutral 1 636)
  - Test: 5 756 tweets (Negative 1 962, Positive 1 949, Neutral 1 845)
- Tweet length: mean ≈75 chars across splits (min 0 after aggressive cleaning — safe to drop zero-length rows if desired).

## 3. Preprocessing & Feature Pipeline
- Classical models use `load_and_preprocess_data()` to apply regex cleaning, stopword removal, and label encoding.
- TF–IDF vectorisation + model training handled in a `Pipeline` (see `traditional_models.py:30`).
- DistilBERT fine-tuning occurs in `transformer_models.py` with HuggingFace `Trainer`; tokenisation uses max-length padding/truncation (`_tokenize_function`).

## 4. Experimental Setup
- Classical models trained on the full processed splits.
- DistilBERT default run capped at `MAX_TRAIN_SAMPLES=3000`, `MAX_EVAL_SAMPLES=2000`, `NUM_TRAIN_EPOCHS=1`, `TRAIN_BATCH_SIZE=16`, `EVAL_BATCH_SIZE=32` to keep CPU runtime under ~15 minutes (configurable via environment variables).
- Reporting script `analysis/generate_report.py` orchestrates the end-to-end workflow, emitting JSON metrics and PNG charts in `reports/`.
- Command to regenerate everything:
  ```bash
  python3 analysis/generate_report.py
  ```
  (override env vars for larger transformer runs before invoking).

## 5. Results Summary
### Classical Models (full dataset)
| Model | Train Accuracy | Train Macro F1 | Validation Accuracy | Validation Macro F1 | Test Accuracy | Test Macro F1 |
| --- | --- | --- | --- | --- | --- | --- |
| Naive Bayes | 0.858 | 0.855 | 0.623 | 0.596 | 0.594 | 0.574 |
| Logistic Regression | 0.907 | 0.906 | 0.632 | 0.619 | 0.611 | 0.606 |
| Linear SVM | 0.959 | 0.960 | 0.606 | 0.594 | 0.577 | 0.570 |

### DistilBERT (subsampled run)
- Train samples: 3 000 (subset of training split)
- Validation/Test samples: 2 000 each (subset for quick evaluation)
- Training accuracy: 0.722 (macro F1 0.702)
- Validation accuracy: 0.641 (macro F1 0.611)
- Test accuracy: 0.608 (macro F1 0.585)

> Note: Running DistilBERT on the full 46 K examples would take ~7–8 hours for 2 epochs on the M4 Air CPU but should tighten the gap further.

## 6. Visual Insights (see `reports/figures/`)
- `label_distribution.png`: class balance is relatively even, slightly skewed toward Negative.
- `text_length_distribution.png`: train/val/test share similar length profiles, confirming split consistency.
- `model_*` charts: compare training/validation/test accuracy and macro metrics for every model—classical baselines nearly memorize the training data, while DistilBERT stays closer to validation/test scores.

## 7. Key Takeaways
- Cleaning/reshuffling eliminated data leakage, reducing inflated classical scores but yielding realistic baselines.
- TF–IDF + Logistic Regression remains a strong lightweight baseline (0.61 test accuracy) for rapid experimentation.
- Training metrics reveal classical models nearly memorize the dataset (≈0.95 training accuracy) while transformers stay closer to held-out scores—use validation/test numbers for real performance.
- Even with a small subsample DistilBERT surpasses classical models, confirming transformer benefits on nuanced sentiment labels.
- Runtime constraints on CPU require subsampling or patience; consider GPU or gradient-accumulation tweaks for full-scale runs.

## 8. Next Steps
1. Run full-data DistilBERT fine-tuning (set `MAX_TRAIN_SAMPLES`/`MAX_EVAL_SAMPLES` to full sizes) and log training curves.
2. Explore more advanced transformers (e.g., RoBERTa, DeBERTa) with mixed precision on GPU.
3. Introduce cross-validation for classical models and calibrate decision thresholds for neutral vs. polar sentiments.
4. Extend reporting to include confusion matrices, per-entity breakdowns, and latency benchmarks.

## 9. Reproducibility Checklist
- Install deps: `pip install -r requirements.txt`
- Regenerate processed dataset: `python3 prepare_data.py`
- Run experiments: `python3 main.py` (set env vars for transformer size/epochs)
- Produce report: `python3 analysis/generate_report.py`
- Inspect outputs in `reports/` for latest metrics & visuals.
